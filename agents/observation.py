"""
Citation ê¸°ëŠ¥ì´ ê°•í™”ëœ Observation Agent
KB ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ëª…í™•í•œ Citation í‘œì‹œ
"""

import json
from typing import Dict, List, Any
from utils.config import AgentConfig
from utils.bedrock_client import BedrockClient


class CitationEnhancedObservationAgent:
    """
    Citation ê¸°ëŠ¥ì´ ê°•í™”ëœ Observation Agent
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    - KB ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ëª…í™•í•œ Citation í‘œì‹œ
    - ì¶œì²˜ ì •ë³´ ìë™ í¬í•¨
    - ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„± ê°•í™”
    """
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.bedrock_client = BedrockClient()
    
    def observe(self, context: Dict, previous_steps: List) -> Dict:
        """Citationì´ ê°•í™”ëœ ê²°ê³¼ ë¶„ì„ ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
        try:
            # ê°€ì¥ ìµœê·¼ Action ê²°ê³¼ ì°¾ê¸°
            action_result = None
            orchestration_result = None
            
            for step in reversed(previous_steps):
                if step.get("type") == "Action" and action_result is None:
                    action_result = step
                elif step.get("type") == "Orchestration" and orchestration_result is None:
                    orchestration_result = step
                
                if action_result and orchestration_result:
                    break
            
            # Orchestrationì—ì„œ ë§¥ë½ ì •ë³´ ì¶”ì¶œ
            context_info = self._extract_context_info(orchestration_result)
            
            if not action_result:
                # Actionì´ ì—†ëŠ” ê²½ìš° (KB ê²€ìƒ‰ ë¶ˆí•„ìš”í•œ ê²½ìš°)
                return self._handle_no_action_case_with_context(context, previous_steps, context_info)
            
            # Action ê²°ê³¼ ë¶„ì„
            search_results = action_result.get("search_results", [])
            action_error = action_result.get("parsed_result", {}).get("error", False)
            search_keywords = action_result.get("parsed_result", {}).get("search_keywords", [])
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° - Citation ê°•í™”
            if search_results and not action_error:
                return self._analyze_search_results_with_enhanced_citation(search_results, context, search_keywords, context_info)
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì§€ë§Œ ì˜¤ë¥˜ê°€ ì•„ë‹Œ ê²½ìš°
            elif not search_results and not action_error:
                return self._handle_no_search_case_with_context(context, previous_steps, context_info)
            
            # ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜ì¸ ê²½ìš°
            else:
                return self._handle_search_failure_with_context(action_result, context, search_keywords, context_info)
                
        except Exception as e:
            return {
                "type": "Observation",
                "model": self.config.observation_model,
                "content": f"Observation ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "parsed_result": {
                    "analysis": "ì˜¤ë¥˜ ë°œìƒ",
                    "is_final_answer": True,
                    "final_answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "needs_retry": False,
                    "retry_keywords": []
                },
                "error": True
            }
    
    def _analyze_search_results_with_enhanced_citation(self, search_results: List[Dict], context: Dict, search_keywords: List[str], context_info: Dict) -> Dict:
        """Citationì´ ê°•í™”ëœ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
        try:
            original_query = context.get("original_query", "")
            conversation_history = context.get("conversation_history", [])
            
            # Citation ì •ë³´ ì¤€ë¹„
            citations = []
            results_with_citations = []
            
            for i, result in enumerate(search_results):
                citation_id = i + 1
                citation_info = {
                    "id": citation_id,
                    "content": result.get('content', ''),
                    "source": result.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜'),
                    "score": result.get('score', 0)
                }
                citations.append(citation_info)
                
                # ê²€ìƒ‰ ê²°ê³¼ì— Citation ID ì¶”ê°€
                enhanced_result = result.copy()
                enhanced_result['citation_id'] = citation_id
                results_with_citations.append(enhanced_result)
            
            # í˜„ì¬ ë°˜ë³µ íšŸìˆ˜ í™•ì¸ (previous_stepsì—ì„œ Action ë‹¨ê³„ ìˆ˜ ê³„ì‚°)
            previous_steps = context.get("previous_steps", [])
            action_count = sum(1 for step in previous_steps if step.get("type") == "Action")
            current_iteration = action_count
            
            print(f"   ğŸ“Š í˜„ì¬ ë°˜ë³µ: {current_iteration}íšŒì°¨, ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            
            # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
            quality_assessment = self._assess_search_quality(search_results, original_query, search_keywords, current_iteration)
            
            print(f"   ğŸ¯ í’ˆì§ˆ í‰ê°€: {quality_assessment['reason']} (ì ìˆ˜: {quality_assessment['score']:.3f})")
            
            # ì¬ì‹œë„ê°€ í•„ìš”í•œ ê²½ìš° (5íšŒì°¨ê°€ ì•„ë‹ˆê³  í’ˆì§ˆì´ ë¶ˆì¶©ë¶„í•œ ê²½ìš°)
            if current_iteration < 5 and quality_assessment["needs_retry"]:
                retry_keywords = self._generate_retry_keywords(original_query, search_keywords, quality_assessment["reason"])
                print(f"   ğŸ”„ ì¬ì‹œë„ í‚¤ì›Œë“œ: {retry_keywords}")
                
                return {
                    "type": "Observation",
                    "model": self.config.observation_model,
                    "content": f"ê²€ìƒ‰ ê²°ê³¼ ë¶ˆì¶©ë¶„. ì¬ì‹œë„ í•„ìš”: {retry_keywords}",
                    "parsed_result": {
                        "analysis": f"ê²€ìƒ‰ ê²°ê³¼ ë¶ˆì¶©ë¶„ ({current_iteration}íšŒì°¨): {quality_assessment['reason']}",
                        "is_final_answer": False,
                        "final_answer": "",
                        "needs_retry": True,
                        "retry_keywords": retry_keywords,
                        "retry_reason": quality_assessment["reason"],
                        "iteration_count": current_iteration
                    },
                    "search_results_count": len(search_results),
                    "quality_score": quality_assessment["score"],
                    "error": False
                }
            
            # ìµœì¢… ë‹µë³€ ìƒì„± (5íšŒì°¨ì´ê±°ë‚˜ í’ˆì§ˆì´ ì¶©ë¶„í•œ ê²½ìš°)
            print(f"   âœ… ìµœì¢… ë‹µë³€ ìƒì„± (ë°˜ë³µ: {current_iteration}, í’ˆì§ˆ ì¶©ì¡±: {not quality_assessment['needs_retry']})")
            
            # ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸ êµ¬ì„± (Citation í¬í•¨)
            results_text = ""
            for citation in citations:
                content = citation['content'][:400]  # 400ìë¡œ ì œí•œ
                results_text += f"[{citation['id']}] {content}...\nì¶œì²˜: {citation['source']}\n\n"
            
            # ì´ì „ ëŒ€í™” ë§¥ë½
            previous_context = ""
            if context_info.get("has_context", False) and conversation_history:
                recent_messages = conversation_history[-2:]
                for msg in recent_messages:
                    if msg.get("role") == "user":
                        previous_context += f"ì´ì „ ì§ˆë¬¸: {msg.get('content', '')}\n"
                    elif msg.get("role") == "assistant":
                        previous_context += f"ì´ì „ ë‹µë³€: {msg.get('content', '')[:150]}...\n"
            
            # Citation ê°•í™” í”„ë¡¬í”„íŠ¸ (ìµœì¢… ë‹µë³€ìš©)
            if current_iteration >= 5:
                prompt = f"""ì´ì „ ëŒ€í™” ë§¥ë½:
{previous_context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {original_query}
ë¶„ì„ëœ ì˜ë„: {context_info.get('intent', '')}

Knowledge Base ê²€ìƒ‰ ê²°ê³¼ ({current_iteration}íšŒ ê²€ìƒ‰ í›„):
{results_text}

**ì¤‘ìš” ì§€ì¹¨:**
ì´ê²ƒì€ {current_iteration}íšŒì°¨ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ì™„ë²½í•˜ì§€ ì•Šë”ë¼ë„ ê°€ëŠ¥í•œ í•œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

1. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ì„±ì´ ìˆëŠ” ë¶€ë¶„ì„ ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”
2. ì§ì ‘ì ì¸ ì •ë³´ê°€ ì—†ë”ë¼ë„ ìœ ì‚¬í•œ ë‚´ìš©ì´ë‚˜ ì¼ë°˜ì ì¸ ì ˆì°¨ë¥¼ ì„¤ëª…í•˜ì„¸ìš”
3. ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë¥¼ í™œìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [1], [2] í˜•íƒœì˜ Citationì„ í¬í•¨í•˜ì„¸ìš”
4. ë‹µë³€ ë§ˆì§€ë§‰ì— "**ì°¸ê³  ìë£Œ:**" ì„¹ì…˜ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë“  ì¶œì²˜ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”
5. ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œí•˜ê³  ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”

ë‹µë³€:"""
            else:
                prompt = f"""ì´ì „ ëŒ€í™” ë§¥ë½:
{previous_context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {original_query}
ë¶„ì„ëœ ì˜ë„: {context_info.get('intent', '')}

Knowledge Base ê²€ìƒ‰ ê²°ê³¼:
{results_text}

ìœ„ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì¤‘ìš”í•œ ìš”êµ¬ì‚¬í•­:**
1. ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë¥¼ í™œìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [1], [2] í˜•íƒœì˜ Citationì„ í¬í•¨í•˜ì„¸ìš”
2. ë‹µë³€ ë§ˆì§€ë§‰ì— "**ì°¸ê³  ìë£Œ:**" ì„¹ì…˜ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë“  ì¶œì²˜ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”
3. ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”

ë‹µë³€:"""
            
            response = self.bedrock_client.invoke_model(
                model_id=self.config.observation_model,
                prompt=prompt,
                temperature=self.config.temperature,
                max_tokens=self.config.get_max_tokens_for_model(self.config.observation_model),
                system_prompt=self._get_citation_system_prompt()
            )
            
            # Citation ì •ë³´ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ì¶”ê°€
            enhanced_response = self._ensure_citations_in_response(response, citations)
            
            return {
                "type": "Observation",
                "model": self.config.observation_model,
                "content": enhanced_response,
                "parsed_result": {
                    "analysis": f"Citation ê°•í™” ê²€ìƒ‰ ê²°ê³¼ {len(search_results)}ê°œ ë¶„ì„ ({current_iteration}íšŒì°¨)",
                    "is_final_answer": True,
                    "final_answer": enhanced_response,
                    "needs_retry": False,
                    "retry_keywords": [],
                    "citations": citations,
                    "search_results_used": len(search_results),
                    "iteration_count": current_iteration
                },
                "search_results_count": len(search_results),
                "context_applied": context_info.get("has_context", False),
                "citations": citations,
                "quality_score": quality_assessment["score"],
                "error": False
            }
            
        except Exception as e:
            return self._create_error_response(f"Citation ê°•í™” ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _ensure_citations_in_response(self, response: str, citations: List[Dict]) -> str:
        """ì‘ë‹µì— Citationì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€"""
        # Citation íŒ¨í„´ í™•ì¸
        import re
        citation_pattern = r'\[\d+\]'
        has_citations = bool(re.search(citation_pattern, response))
        
        # ì°¸ê³  ìë£Œ ì„¹ì…˜ í™•ì¸
        has_references = "ì°¸ê³  ìë£Œ" in response or "References" in response or "ì¶œì²˜" in response
        
        if not has_citations or not has_references:
            # Citation ìë™ ì¶”ê°€
            enhanced_response = response
            
            if not has_references:
                enhanced_response += "\n\n**ì°¸ê³  ìë£Œ:**\n"
                for citation in citations:
                    enhanced_response += f"[{citation['id']}] {citation['source']}\n"
            
            return enhanced_response
        
        return response
    
    def _get_citation_system_prompt(self) -> str:
        """Citation ê°•í™” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        base_prompt = self.config.system_prompt or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
        
        return f"""{base_prompt}

ë‹¹ì‹ ì€ Knowledge Base ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**Citation ê·œì¹™:**
1. ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [1], [2] í˜•íƒœë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”
2. ë‹µë³€ ë§ˆì§€ë§‰ì— "**ì°¸ê³  ìë£Œ:**" ì„¹ì…˜ì„ í¬í•¨í•˜ì„¸ìš”
3. ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”"""
    
    def _extract_context_info(self, orchestration_result: Dict) -> Dict:
        """Orchestration ê²°ê³¼ì—ì„œ ë§¥ë½ ì •ë³´ ì¶”ì¶œ"""
        if not orchestration_result:
            return {"has_context": False}
        
        parsed_result = orchestration_result.get("parsed_result", {})
        
        return {
            "has_context": parsed_result.get("context_applied", False),
            "intent": parsed_result.get("intent", ""),
            "keywords": parsed_result.get("search_keywords", []),
            "confidence": parsed_result.get("confidence", 0)
        }
    
    def _handle_no_action_case_with_context(self, context: Dict, previous_steps: List, context_info: Dict) -> Dict:
        """ë§¥ë½ì„ ê³ ë ¤í•œ Action ì—†ëŠ” ê²½ìš° ì²˜ë¦¬"""
        try:
            original_query = context.get("original_query", "")
            conversation_history = context.get("conversation_history", [])
            
            # ê°„ë‹¨í•œ ì¸ì‚¬ë§ ì²˜ë¦¬
            if self._is_simple_greeting(original_query):
                answer = self._generate_greeting_response(original_query)
                return self._create_final_response(answer, "ê°„ë‹¨í•œ ì¸ì‚¬ë§ë¡œ ì§ì ‘ ë‹µë³€ ìƒì„±")
            
            # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„±
            if context_info.get("has_context", False):
                return self._generate_context_aware_answer(original_query, conversation_history, context_info)
            else:
                return self._generate_general_answer(original_query)
            
        except Exception as e:
            return self._create_error_response(f"ì§ì ‘ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _generate_context_aware_answer(self, query: str, history: List[Dict], context_info: Dict) -> Dict:
        """ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ ìƒì„± - ëŒ€í™” ì—°ì†ì„± ê°•í™”"""
        try:
            # ì´ì „ ëŒ€í™”ì—ì„œ ë§¥ë½ ì¶”ì¶œ (ë” ë§ì€ ëŒ€í™” í¬í•¨)
            previous_context = ""
            if history:
                # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ê¹Œì§€ í¬í•¨ (ë” í’ë¶€í•œ ë§¥ë½)
                recent_messages = history[-6:]
                for msg in recent_messages:
                    role = msg.get("role", "")
                    content = msg.get("content", "")
                    if role == "user":
                        previous_context += f"ì‚¬ìš©ì: {content}\n"
                    elif role == "assistant":
                        # ì´ì „ ë‹µë³€ì„ ë” ê¸¸ê²Œ í¬í•¨ (500ìê¹Œì§€)
                        previous_context += f"ì–´ì‹œìŠ¤í„´íŠ¸: {content[:500]}...\n"
            
            # ì—°ì†ì„± ì§ˆë¬¸ ê°ì§€
            is_continuation = self._is_conversation_continuation_question(query)
            
            # ë§¥ë½ ì¸ì‹ í”„ë¡¬í”„íŠ¸ (ë” ìƒì„¸í•˜ê²Œ)
            if is_continuation:
                prompt = f"""ì´ì „ ëŒ€í™” ë§¥ë½:
{previous_context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ì˜ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨:**
1. ì´ì „ ëŒ€í™”ì˜ ë‚´ìš©ì„ ì •í™•íˆ ê¸°ì–µí•˜ê³  ì°¸ì¡°í•˜ì„¸ìš”
2. "ë‹¤ìŒì€?", "ê·¸ëŸ¼?", "ë˜ëŠ”?" ê°™ì€ ì—°ì†ì„± ì§ˆë¬¸ì˜ ê²½ìš°, ì´ì „ ë‹µë³€ì˜ ë‚´ìš©ì„ ì´ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
3. ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰í–ˆë˜ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ì„¸ìš”
4. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ì„ ìœ ì§€í•˜ì„¸ìš”

ë‹µë³€:"""
            else:
                prompt = f"""ì´ì „ ëŒ€í™” ë§¥ë½:
{previous_context}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë¶„ì„ëœ ì˜ë„: {context_info.get('intent', '')}

ìœ„ì˜ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ì „ ëŒ€í™”ì™€ ê´€ë ¨ì´ ìˆë‹¤ë©´ ê·¸ ì—°ê´€ì„±ì„ ì–¸ê¸‰í•˜ë©´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
            
            response = self.bedrock_client.invoke_model(
                model_id=self.config.observation_model,
                prompt=prompt,
                temperature=self.config.temperature,
                max_tokens=min(2000, self.config.get_max_tokens_for_model(self.config.observation_model)),  # ë” ê¸´ ë‹µë³€ í—ˆìš©
                system_prompt=self._get_conversation_system_prompt()
            )
            
            return self._create_final_response(
                response.strip(), 
                f"ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ ìƒì„± (ì—°ì†ì„±: {is_continuation}, ì˜ë„: {context_info.get('intent', 'N/A')})"
            )
            
        except Exception as e:
            return self._create_error_response(f"ë§¥ë½ ì¸ì‹ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _is_conversation_continuation_question(self, query: str) -> bool:
        """ëŒ€í™” ì—°ì†ì„± ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
        continuation_patterns = [
            "ë‹¤ìŒì€", "ê·¸ëŸ¼", "ê·¸ëŸ¬ë©´", "ë˜ëŠ”", "ì•„ë‹ˆë©´", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ°ë°",
            "ê·¸ë˜ì„œ", "ê·¸ë ‡ë‹¤ë©´", "ê³„ì†", "ì´ì–´ì„œ", "ì¶”ê°€ë¡œ", "ë”", "ë˜", "ê·¸ ì™¸ì—"
        ]
        
        query_lower = query.lower().strip()
        return any(pattern in query_lower for pattern in continuation_patterns)
    
    def _get_conversation_system_prompt(self) -> str:
        """ëŒ€í™” ë§¥ë½ ì¸ì‹ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        base_prompt = self.config.system_prompt or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
        
        return f"""{base_prompt}

ë‹¹ì‹ ì€ ëŒ€í™”ì˜ ë§¥ë½ì„ ì •í™•íˆ ê¸°ì–µí•˜ê³  ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ëŒ€í™” ì—°ì†ì„± ê·œì¹™:**
1. ì´ì „ ëŒ€í™”ì˜ ë‚´ìš©ì„ ì •í™•íˆ ê¸°ì–µí•˜ê³  ì°¸ì¡°í•˜ì„¸ìš”
2. ì—°ì†ì„± ì§ˆë¬¸("ë‹¤ìŒì€?", "ê·¸ëŸ¼?" ë“±)ì—ëŠ” ì´ì „ ë‹µë³€ì„ ì´ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
3. ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•œ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥í•˜ê±°ë‚˜ ë³´ì™„í•˜ì„¸ìš”
4. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ì„ ìœ ì§€í•˜ì„¸ìš”
5. ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„±ì„ ëª…í™•íˆ í‘œí˜„í•˜ì„¸ìš”"""
    
    def _generate_general_answer(self, query: str) -> Dict:
        """ì¼ë°˜ì ì¸ ë‹µë³€ ìƒì„±"""
        try:
            prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì´ ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
            
            response = self.bedrock_client.invoke_model(
                model_id=self.config.observation_model,
                prompt=prompt,
                temperature=self.config.temperature,
                max_tokens=min(1000, self.config.get_max_tokens_for_model(self.config.observation_model)),
                system_prompt=self.config.system_prompt
            )
            
            return self._create_final_response(response.strip(), "ì¼ë°˜ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì§ì ‘ ë‹µë³€ ìƒì„±")
            
        except Exception as e:
            return self._create_error_response(f"ì¼ë°˜ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _handle_no_search_case_with_context(self, context: Dict, previous_steps: List, context_info: Dict) -> Dict:
        """ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬"""
        return self._handle_no_action_case_with_context(context, previous_steps, context_info)
    
    def _handle_search_failure_with_context(self, action_result: Dict, context: Dict, search_keywords: List[str], context_info: Dict) -> Dict:
        """ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì‹¤íŒ¨ ì²˜ë¦¬"""
        try:
            original_query = context.get("original_query", "")
            
            # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œì—ë„ ì¼ë°˜ì ì¸ ë‹µë³€ ì œê³µ
            prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {original_query}
ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}

Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”:
1. êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì•ˆë‚´
2. ì¼ë°˜ì ì¸ ê°€ì´ë“œë¼ì¸ì´ë‚˜ ì ˆì°¨ ì•ˆë‚´
3. ì¶”ê°€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ë°©ë²• ì œì•ˆ

ë‹µë³€:"""
            
            response = self.bedrock_client.invoke_model(
                model_id=self.config.observation_model,
                prompt=prompt,
                temperature=self.config.temperature,
                max_tokens=min(1200, self.config.get_max_tokens_for_model(self.config.observation_model)),
                system_prompt=self.config.system_prompt
            )
            
            return self._create_final_response(
                response.strip(),
                "ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ì ì¸ ê°€ì´ë“œë¼ì¸ ì œê³µ"
            )
            
        except Exception as e:
            return self._create_error_response(f"ê²€ìƒ‰ ì‹¤íŒ¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _is_simple_greeting(self, query: str) -> bool:
        """ê°„ë‹¨í•œ ì¸ì‚¬ë§ì¸ì§€ í™•ì¸"""
        greetings = [
            "ì•ˆë…•", "hello", "hi", "hey", "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ",
            "ì¢‹ì€ ì•„ì¹¨", "ì¢‹ì€ ì˜¤í›„", "ì¢‹ì€ ì €ë…", "ë°˜ê°‘ìŠµë‹ˆë‹¤"
        ]
        
        query_lower = query.lower().strip()
        return any(greeting in query_lower for greeting in greetings)
    
    def _generate_greeting_response(self, query: str) -> str:
        """ì¸ì‚¬ë§ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        responses = {
            "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            "hello": "Hello! How can I help you?",
            "hi": "Hi there! What can I do for you?",
            "ì•ˆë…•í•˜ì„¸ìš”": "ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”.",
            "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ": "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ! ë„ì›€ì´ í•„ìš”í•œ ì¼ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        }
        
        query_lower = query.lower().strip()
        
        for greeting, response in responses.items():
            if greeting in query_lower:
                return response
        
        return "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    
    def _assess_search_quality(self, search_results: List[Dict], query: str, keywords: List[str], iteration: int = 1) -> Dict:
        """LLM ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        if not search_results or len(search_results) == 0:
            return {
                "needs_retry": iteration < 5,
                "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                "score": 0.0
            }
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
            results_summary = ""
            for i, result in enumerate(search_results, 1):
                content = result.get('content', '')[:200]  # 200ìë¡œ ì œí•œ
                score = result.get('score', 0)
                results_summary += f"ê²°ê³¼ {i} (ê´€ë ¨ì„±: {score:.3f}): {content}...\n"
            
            # LLMì„ í†µí•œ í’ˆì§ˆ í‰ê°€
            evaluation_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}
ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords}
í˜„ì¬ ë°˜ë³µ íšŸìˆ˜: {iteration}/5

ê²€ìƒ‰ ê²°ê³¼:
{results_summary}

ìœ„ì˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸°ì— ì¶©ë¶„í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± (ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆëŠ”ê°€?)
2. ì •ë³´ì˜ êµ¬ì²´ì„± (êµ¬ì²´ì ì¸ ì ˆì°¨, ê·œì •, ê¸°ì¤€ ë“±ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?)
3. ë‹µë³€ ì™„ì„±ë„ (ì´ ì •ë³´ë§Œìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ”ê°€?)

ë°˜ë³µ íšŸìˆ˜ë³„ ê¸°ì¤€:
- 1-2íšŒì°¨: ì—„ê²©í•œ ê¸°ì¤€ (ë†’ì€ ê´€ë ¨ì„±ê³¼ êµ¬ì²´ì  ì •ë³´ í•„ìš”)
- 3-4íšŒì°¨: ì™„í™”ëœ ê¸°ì¤€ (ë¶€ë¶„ì  ì •ë³´ë¼ë„ ìœ ìš©í•˜ë©´ í†µê³¼)
- 5íšŒì°¨: ë§¤ìš° ì™„í™”ëœ ê¸°ì¤€ (ìµœì†Œí•œì˜ ê´€ë ¨ ì •ë³´ë§Œ ìˆì–´ë„ í†µê³¼)

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
QUALITY_SCORE: [0.0-1.0 ì‚¬ì´ì˜ ì ìˆ˜]
SUFFICIENT: [YES/NO]
REASON: [í‰ê°€ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ]"""

            response = self.bedrock_client.invoke_model(
                model_id=self.config.observation_model,
                prompt=evaluation_prompt,
                temperature=0.1,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
                max_tokens=200,
                system_prompt="ë‹¹ì‹ ì€ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            )
            
            # ì‘ë‹µ íŒŒì‹±
            lines = response.strip().split('\n')
            quality_score = 0.0
            is_sufficient = False
            reason = "í‰ê°€ ì‹¤íŒ¨"
            
            for line in lines:
                if line.startswith('QUALITY_SCORE:'):
                    try:
                        quality_score = float(line.split(':')[1].strip())
                    except:
                        quality_score = 0.0
                elif line.startswith('SUFFICIENT:'):
                    is_sufficient = 'YES' in line.upper()
                elif line.startswith('REASON:'):
                    reason = line.split(':', 1)[1].strip()
            
            # ìµœì¢… ë°˜ë³µ(5íšŒì°¨)ì—ì„œëŠ” í•­ìƒ ì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨
            if iteration >= 5:
                is_sufficient = True
                reason += " (ìµœì¢… ë°˜ë³µ)"
            
            needs_retry = not is_sufficient and iteration < 5
            
            print(f"   ğŸ¤– LLM í’ˆì§ˆ í‰ê°€: ì ìˆ˜={quality_score:.3f}, ì¶©ë¶„={is_sufficient}, ì¬ì‹œë„={needs_retry}")
            
            return {
                "needs_retry": needs_retry,
                "reason": reason,
                "score": quality_score
            }
            
        except Exception as e:
            print(f"   âš ï¸ LLM í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {str(e)}")
            # í´ë°±: ê°„ë‹¨í•œ ì ìˆ˜ ê¸°ë°˜ í‰ê°€
            avg_score = sum(result.get('score', 0) for result in search_results) / len(search_results)
            
            # ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¥¸ ê¸°ë³¸ ì„ê³„ê°’
            threshold = 0.5 if iteration <= 2 else (0.4 if iteration <= 4 else 0.2)
            
            return {
                "needs_retry": avg_score < threshold and iteration < 5,
                "reason": f"í´ë°± í‰ê°€: í‰ê·  ì ìˆ˜ {avg_score:.3f} (ì„ê³„ê°’: {threshold})",
                "score": avg_score
            }
    
    def _generate_retry_keywords(self, query: str, previous_keywords: List[str], reason: str) -> List[str]:
        """LLM ê¸°ë°˜ ì¬ì‹œë„ í‚¤ì›Œë“œ ìƒì„±"""
        try:
            keyword_generation_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}
ì´ì „ ê²€ìƒ‰ í‚¤ì›Œë“œ: {previous_keywords}
ê²€ìƒ‰ ì‹¤íŒ¨ ì´ìœ : {reason}

ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ í‚¤ì›Œë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

í‚¤ì›Œë“œ ìƒì„± ì „ëµ:
1. ë™ì˜ì–´/ìœ ì‚¬ì–´ ì‚¬ìš© (ì˜ˆ: "ê·œì •" â†’ "ì •ì±…", "ì§€ì¹¨", "ê¸°ì¤€")
2. ë” êµ¬ì²´ì ì¸ ìš©ì–´ ì‚¬ìš© (ì˜ˆ: "ì§€ì›" â†’ "ì§€ì›ê¸ˆ", "ì§€ì›ì œë„")
3. ë” ì¼ë°˜ì ì¸ ìš©ì–´ ì‚¬ìš© (ë„ˆë¬´ êµ¬ì²´ì ì´ì—ˆë‹¤ë©´)
4. ê´€ë ¨ ë¶„ì•¼ì˜ ì „ë¬¸ ìš©ì–´ í™œìš©
5. ë‹¤ë¥¸ í‘œí˜„ ë°©ì‹ ì‹œë„

ì´ì „ í‚¤ì›Œë“œì™€ëŠ” ë‹¤ë¥¸ ìƒˆë¡œìš´ í‚¤ì›Œë“œ 3-4ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

í˜•ì‹:
KEYWORD1: [í‚¤ì›Œë“œ1]
KEYWORD2: [í‚¤ì›Œë“œ2]
KEYWORD3: [í‚¤ì›Œë“œ3]
KEYWORD4: [í‚¤ì›Œë“œ4] (ì„ íƒì‚¬í•­)"""

            response = self.bedrock_client.invoke_model(
                model_id=self.config.observation_model,
                prompt=keyword_generation_prompt,
                temperature=0.7,  # ì°½ì˜ì ì¸ í‚¤ì›Œë“œ ìƒì„±ì„ ìœ„í•´ ë†’ì€ temperature
                max_tokens=300,
                system_prompt="ë‹¹ì‹ ì€ ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê´€ì ì—ì„œ íš¨ê³¼ì ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            new_keywords = []
            lines = response.strip().split('\n')
            
            for line in lines:
                if line.startswith('KEYWORD'):
                    try:
                        keyword = line.split(':', 1)[1].strip()
                        if keyword and keyword not in new_keywords and keyword not in str(previous_keywords):
                            new_keywords.append(keyword)
                    except:
                        continue
            
            # í‚¤ì›Œë“œê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° í´ë°±
            if not new_keywords:
                print("   âš ï¸ LLM í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨, í´ë°± ì‚¬ìš©")
                return self._fallback_keyword_generation(query, previous_keywords)
            
            print(f"   ğŸ¤– LLM ìƒì„± í‚¤ì›Œë“œ: {new_keywords}")
            return new_keywords[:4]  # ìµœëŒ€ 4ê°œ
            
        except Exception as e:
            print(f"   âš ï¸ LLM í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return self._fallback_keyword_generation(query, previous_keywords)
    
    def _fallback_keyword_generation(self, query: str, previous_keywords: List[str]) -> List[str]:
        """í´ë°± í‚¤ì›Œë“œ ìƒì„± (LLM ì‹¤íŒ¨ ì‹œ)"""
        import re
        
        # ì›ë³¸ ì¿¼ë¦¬ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
        korean_words = re.findall(r'[ê°€-í£]+', query)
        new_keywords = []
        
        # ë‹¨ì–´ ì¡°í•© ìƒì„±
        if len(korean_words) >= 2:
            for i in range(len(korean_words)-1):
                combined = f"{korean_words[i]} {korean_words[i+1]}"
                if combined not in str(previous_keywords):
                    new_keywords.append(combined)
        
        # ê°œë³„ ë‹¨ì–´ ì¶”ê°€
        for word in korean_words:
            if len(word) >= 2 and word not in str(previous_keywords):
                new_keywords.append(word)
        
        # ì›ë³¸ ì¿¼ë¦¬ ì¼ë¶€ ì‚¬ìš©
        if not new_keywords:
            new_keywords = [query[:15], query[-15:]]
        
        return new_keywords[:3]
    
    def _create_final_response(self, answer: str, analysis: str) -> Dict:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        return {
            "type": "Observation",
            "model": self.config.observation_model,
            "content": answer,
            "parsed_result": {
                "analysis": analysis,
                "is_final_answer": True,
                "final_answer": answer,
                "needs_retry": False,
                "retry_keywords": []
            },
            "error": False
        }
    
    def _create_error_response(self, error_msg: str) -> Dict:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            "type": "Observation",
            "model": self.config.observation_model,
            "content": error_msg,
            "parsed_result": {
                "analysis": "ì—ëŸ¬ ë°œìƒ",
                "is_final_answer": True,
                "final_answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "needs_retry": False,
                "retry_keywords": []
            },
            "error": True
        }
    
    def get_model_name(self) -> str:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ëª… ë°˜í™˜"""
        model_id = self.config.observation_model
        if "claude-sonnet-4" in model_id:
            return "Claude 4 (Citation-Enhanced)"
        elif "claude-3-7-sonnet" in model_id:
            return "Claude 3.7 Sonnet (Citation-Enhanced)"
        elif "claude-3-5-sonnet" in model_id:
            return "Claude 3.5 Sonnet (Citation-Enhanced)"
        elif "claude-3-5-haiku" in model_id:
            return "Claude 3.5 Haiku (Citation-Enhanced)"
        elif "nova-lite" in model_id:
            return "Nova Lite (Citation-Enhanced)"
        elif "nova-micro" in model_id:
            return "Nova Micro (Citation-Enhanced)"
        else:
            return f"{model_id.split(':')[0]} (Citation-Enhanced)"


# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ContextAwareObservationAgent = CitationEnhancedObservationAgent
ObservationAgent = CitationEnhancedObservationAgent  # react_agent.py í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€
