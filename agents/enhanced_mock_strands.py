"""
ì‹¤ì œ KB ê²€ìƒ‰ì´ ê°€ëŠ¥í•œ í–¥ìƒëœ Mock Strands Agents
ì‹¤ì œ Strands ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ë„ KB ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""

import json
import time
from typing import Dict, List, Any, Optional, Callable
from utils.kb_search import KnowledgeBaseSearcher
from utils.bedrock_client import BedrockClient


class EnhancedMockAgent:
    """ì‹¤ì œ KB ê²€ìƒ‰ì´ ê°€ëŠ¥í•œ Mock Agent"""
    
    def __init__(self, config=None, system_prompt=None, tools=None):
        # configê°€ ì§ì ‘ ì „ë‹¬ë˜ê±°ë‚˜ ì²« ë²ˆì§¸ ì¸ìë¡œ ì „ë‹¬ë  ìˆ˜ ìˆìŒ
        if hasattr(config, 'is_kb_enabled'):
            self.config = config
        else:
            # configê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            self.config = config
            
        self.tools = tools or []
        self.kb_searcher = KnowledgeBaseSearcher() if (self.config and self.config.is_kb_enabled()) else None
        self.bedrock_client = BedrockClient()
        self._last_search_results = []  # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ìš©
        
        print(f"ğŸ¤– Enhanced Mock Agent ì´ˆê¸°í™” (KB: {self.config.is_kb_enabled() if self.config else False})")
    
    def __call__(self, prompt: str) -> str:
        """Agent í˜¸ì¶œ - ì‹¤ì œ KB ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # KB ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨
            if self._needs_kb_search(prompt):
                return self._process_with_kb_search(prompt)
            else:
                return self._process_simple_response(prompt)
                
        except Exception as e:
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _needs_kb_search(self, prompt: str, conversation_history: List[Dict] = None) -> bool:
        """KB ê²€ìƒ‰ì´ í•„ìš”í•œì§€ LLM ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨"""
        if not self.config.is_kb_enabled():
            return False
        
        conversation_history = conversation_history or []
        
        try:
            # ëŒ€í™” ì—°ì†ì„± í™•ì¸
            if self._is_conversation_continuation(prompt, conversation_history):
                print("   ğŸ”— ëŒ€í™” ì—°ì†ì„± ê°ì§€ - KB ê²€ìƒ‰ ë¶ˆí•„ìš”")
                return False
            
            # ê°„ë‹¨í•œ ì¸ì‚¬ë§ í™•ì¸
            if self._is_simple_greeting(prompt):
                print("   ğŸ‘‹ ê°„ë‹¨í•œ ì¸ì‚¬ë§ - KB ê²€ìƒ‰ ë¶ˆí•„ìš”")
                return False
            
            # LLMì„ í†µí•œ KB ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
            decision_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì´ Knowledge Base ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

íŒë‹¨ ê¸°ì¤€:
1. íšŒì‚¬ ì •ì±…, ê·œì •, ì œë„ì— ê´€í•œ ì§ˆë¬¸ â†’ KB ê²€ìƒ‰ í•„ìš”
2. ë³µë¦¬í›„ìƒ, ì§€ì›ì œë„ì— ê´€í•œ ì§ˆë¬¸ â†’ KB ê²€ìƒ‰ í•„ìš”  
3. êµ¬ì²´ì ì¸ ì ˆì°¨ë‚˜ ê¸°ì¤€ì— ê´€í•œ ì§ˆë¬¸ â†’ KB ê²€ìƒ‰ í•„ìš”
4. ì¼ë°˜ì ì¸ ëŒ€í™”, ì¸ì‚¬ë§ â†’ KB ê²€ìƒ‰ ë¶ˆí•„ìš”
5. ê°œë… ì„¤ëª…, ì¼ë°˜ ì§€ì‹ â†’ KB ê²€ìƒ‰ ë¶ˆí•„ìš”

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
NEEDS_KB_SEARCH: [YES/NO]
REASON: [íŒë‹¨ ì´ìœ ]"""

            # Cross Region Inferenceìš© ëª¨ë¸ ID ì‚¬ìš©
            model_id = self.config.observation_model
            if "claude-3-5-haiku" in model_id:
                model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
            elif "claude-3-5-sonnet" in model_id:
                model_id = "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            elif "nova-lite" in model_id:
                model_id = "us.amazon.nova-lite-v1:0"
            elif "nova-micro" in model_id:
                model_id = "us.amazon.nova-micro-v1:0"

            response = self.bedrock_client.invoke_model(
                model_id=model_id,
                prompt=decision_prompt,
                temperature=0.1,  # ì¼ê´€ëœ íŒë‹¨
                max_tokens=150,
                system_prompt="ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ Knowledge Base ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            )
            
            # ì‘ë‹µ íŒŒì‹±
            lines = response.strip().split('\n')
            needs_search = False
            reason = "íŒë‹¨ ì‹¤íŒ¨"
            
            for line in lines:
                if line.startswith('NEEDS_KB_SEARCH:'):
                    needs_search = 'YES' in line.upper()
                elif line.startswith('REASON:'):
                    reason = line.split(':', 1)[1].strip()
            
            print(f"   ğŸ¤– KB ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨: {needs_search} ({reason})")
            return needs_search
            
        except Exception as e:
            print(f"   âš ï¸ KB ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ ì˜¤ë¥˜: {str(e)}")
            # í´ë°±: ê¸°ë³¸ì ì¸ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            return self._fallback_kb_search_decision(prompt, conversation_history)
    
    def _is_conversation_continuation(self, prompt: str, conversation_history: List[Dict]) -> bool:
        """ëŒ€í™” ì—°ì†ì„± í™•ì¸"""
        if not conversation_history:
            return False
        
        # ì—°ì†ì„± í‘œí˜„ í™•ì¸
        continuation_patterns = [
            "ë‹¤ìŒì€", "ê·¸ëŸ¼", "ê·¸ëŸ¬ë©´", "ë˜ëŠ”", "ì•„ë‹ˆë©´", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ°ë°",
            "ê·¸ë˜ì„œ", "ê·¸ë ‡ë‹¤ë©´", "ê³„ì†", "ì´ì–´ì„œ", "ì¶”ê°€ë¡œ", "ë”", "ë˜", "ê·¸ ì™¸ì—",
            "ê·¸ê²ƒ", "ê·¸ê±°", "ì´ê²ƒ", "ì´ê±°", "ìœ„ì—ì„œ", "ì•ì„œ"
        ]
        
        prompt_lower = prompt.lower().strip()
        return any(pattern in prompt_lower for pattern in continuation_patterns)
    
    def _is_simple_greeting(self, prompt: str) -> bool:
        """ê°„ë‹¨í•œ ì¸ì‚¬ë§ í™•ì¸"""
        greetings = [
            "ì•ˆë…•", "hello", "hi", "hey", "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ",
            "ì¢‹ì€ ì•„ì¹¨", "ì¢‹ì€ ì˜¤í›„", "ì¢‹ì€ ì €ë…", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤"
        ]
        
        prompt_lower = prompt.lower().strip()
        return any(greeting in prompt_lower for greeting in greetings)
    
    def _fallback_kb_search_decision(self, prompt: str, conversation_history: List[Dict]) -> bool:
        """í´ë°± KB ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        # ëŒ€í™” ì—°ì†ì„±ì´ ìˆìœ¼ë©´ KB ê²€ìƒ‰ ë¶ˆí•„ìš”
        if self._is_conversation_continuation(prompt, conversation_history):
            return False
        
        # ê°„ë‹¨í•œ ì¸ì‚¬ë§ì´ë©´ KB ê²€ìƒ‰ ë¶ˆí•„ìš”
        if self._is_simple_greeting(prompt):
            return False
        
        # ì§ˆë¬¸ í˜•íƒœì´ê³  êµ¬ì²´ì ì¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ KB ê²€ìƒ‰ í•„ìš”
        if any(char in prompt for char in ["?", "ï¼Ÿ", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””ì„œ", "ì™œ"]):
            return True
        
        # ê¸°ë³¸ì ìœ¼ë¡œ KB ê²€ìƒ‰ ìˆ˜í–‰
        return True
    
    def _process_with_kb_search(self, prompt: str) -> str:
        """KB ê²€ìƒ‰ì„ í¬í•¨í•œ ì²˜ë¦¬"""
        try:
            print(f"ğŸ” KB ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(prompt)
            print(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ: {keywords}")
            
            # KB ê²€ìƒ‰ ì‹¤í–‰
            search_results = self.kb_searcher.search_multiple_queries(
                kb_id=self.config.kb_id,
                queries=keywords,
                max_results_per_query=3
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (Streamlit í˜¸í™˜ì„±ì„ ìœ„í•´)
            self._last_search_results = search_results
            
            print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            
            if search_results:
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                return self._generate_answer_with_kb(prompt, search_results)
            else:
                return self._generate_fallback_answer(prompt)
                
        except Exception as e:
            print(f"âŒ KB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            self._last_search_results = []  # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            return self._generate_fallback_answer(prompt)
    
    def _extract_keywords(self, prompt: str) -> List[str]:
        """LLMì„ í†µí•œ ë™ì  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            keyword_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ Knowledge Base ê²€ìƒ‰ì— íš¨ê³¼ì ì¸ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

í‚¤ì›Œë“œ ì¶”ì¶œ ì§€ì¹¨:
1. ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë…ì„ í¬í•¨í•˜ëŠ” í‚¤ì›Œë“œ
2. êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ ìš©ì–´
3. ë„ˆë¬´ ì¼ë°˜ì ì´ì§€ ì•Šê³  ë„ˆë¬´ êµ¬ì²´ì ì´ì§€ë„ ì•Šì€ ì ì ˆí•œ ìˆ˜ì¤€
4. ë³µí•©ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜ˆ: "ë‚œì„ì¹˜ë£Œì‹œìˆ ë¹„")
5. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ì–´ë³´ë‹¤ëŠ” ì›ë¬¸ì˜ í‘œí˜„ ìš°ì„ 

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
KEYWORD1: [í‚¤ì›Œë“œ1]
KEYWORD2: [í‚¤ì›Œë“œ2]  
KEYWORD3: [í‚¤ì›Œë“œ3]"""

            # Cross Region Inferenceìš© ëª¨ë¸ ID ì‚¬ìš©
            model_id = self.config.observation_model
            if "claude-3-5-haiku" in model_id:
                model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
            elif "claude-3-5-sonnet" in model_id:
                model_id = "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            elif "nova-lite" in model_id:
                model_id = "us.amazon.nova-lite-v1:0"
            elif "nova-micro" in model_id:
                model_id = "us.amazon.nova-micro-v1:0"

            response = self.bedrock_client.invoke_model(
                model_id=model_id,
                prompt=keyword_prompt,
                temperature=0.3,  # ì ë‹¹í•œ ì°½ì˜ì„±
                max_tokens=200,
                system_prompt="ë‹¹ì‹ ì€ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê°€ì¥ íš¨ê³¼ì ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            # í‚¤ì›Œë“œ íŒŒì‹±
            keywords = []
            lines = response.strip().split('\n')
            
            for line in lines:
                if line.startswith('KEYWORD'):
                    try:
                        keyword = line.split(':', 1)[1].strip()
                        if keyword and keyword not in keywords:
                            keywords.append(keyword)
                    except:
                        continue
            
            # LLM ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš° í´ë°±
            if not keywords:
                print("   âš ï¸ LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, í´ë°± ì‚¬ìš©")
                keywords = self._fallback_keyword_extraction(prompt)
            
            print(f"   ğŸ¤– LLM ì¶”ì¶œ í‚¤ì›Œë“œ: {keywords}")
            return keywords[:3]
            
        except Exception as e:
            print(f"   âŒ LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return self._fallback_keyword_extraction(prompt)
    
    def _fallback_keyword_extraction(self, prompt: str) -> List[str]:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        
        # í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        korean_words = [word for word in re.findall(r'[ê°€-í£]+', prompt) if len(word) >= 2]
        
        keywords = []
        
        # 2-3ë‹¨ì–´ ì¡°í•©
        if len(korean_words) >= 2:
            keywords.append(f"{korean_words[0]} {korean_words[1]}")
        
        if len(korean_words) >= 3:
            keywords.append(f"{korean_words[0]} {korean_words[1]} {korean_words[2]}")
        
        # ê°œë³„ ë‹¨ì–´ (ê¸¸ì´ ìˆœ)
        sorted_words = sorted(korean_words, key=len, reverse=True)
        for word in sorted_words[:2]:
            if word not in str(keywords):
                keywords.append(word)
        
        return keywords[:3] if keywords else ["ì •ë³´"]
    
    def _generate_answer_with_kb(self, prompt: str, search_results: List[Dict]) -> str:
        """KB ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± - ê°œì„ ëœ ë²„ì „"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context_parts = []
            sources = []
            
            for i, result in enumerate(search_results[:5], 1):  # ìµœëŒ€ 5ê°œ ê²°ê³¼ ì‚¬ìš©
                content = result.get("content", "")[:400]  # ë” ë§ì€ ë‚´ìš© í¬í•¨
                source = result.get("source", f"ë¬¸ì„œ {i}")
                score = result.get("score", 0)
                
                context_parts.append(f"[{i}] (ê´€ë ¨ì„±: {score:.3f}) {content}")
                sources.append(f"[{i}] {source}")
            
            context = "\n\n".join(context_parts)
            
            # ê°œì„ ëœ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
            answer_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

ê²€ìƒ‰ ê²°ê³¼:
{context}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í˜•íƒœë¡œ êµ¬ì„±
2. ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë¥¼ í™œìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [1], [2] í˜•íƒœì˜ Citation í¬í•¨
3. êµ¬ì²´ì ì¸ ì ˆì°¨, ê¸°ì¤€, ì¡°ê±´, ê¸ˆì•¡ ë“±ì´ ìˆë‹¤ë©´ ëª…í™•íˆ ì„¤ëª…
4. ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰
5. ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
6. ë‹¨ìˆœíˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³  ì§ˆë¬¸ì— ë§ê²Œ ì¬êµ¬ì„±í•˜ì—¬ ë‹µë³€

ì˜ˆì‹œ í˜•íƒœ:
- "~ì— ê´€í•œ ê·œì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: [êµ¬ì²´ì  ë‚´ìš©][1]"
- "ì‹ ì²­ ì ˆì°¨ëŠ” [ë‹¨ê³„ë³„ ì„¤ëª…][2]ì…ë‹ˆë‹¤"
- "ì§€ì› ëŒ€ìƒì€ [ì¡°ê±´ ì„¤ëª…][3]ì— í•´ë‹¹í•˜ëŠ” ê²½ìš°ì…ë‹ˆë‹¤"

ë‹µë³€:"""

            # Cross Region Inferenceìš© ëª¨ë¸ ID ì‚¬ìš©
            model_id = self.config.observation_model
            if "claude-3-5-haiku" in model_id:
                model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
            elif "claude-3-5-sonnet" in model_id:
                model_id = "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            elif "nova-lite" in model_id:
                model_id = "us.amazon.nova-lite-v1:0"
            elif "nova-micro" in model_id:
                model_id = "us.amazon.nova-micro-v1:0"

            # agent ê°ì²´ì˜ bedrock_client ì‚¬ìš©
            response = self.agent.bedrock_client.invoke_model(
                model_id=model_id,
                prompt=answer_prompt,
                temperature=0.1,  # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ temperature
                max_tokens=1500,  # ë” ê¸´ ë‹µë³€ í—ˆìš©
                system_prompt="ë‹¹ì‹ ì€ Knowledge Base ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³  ì§ˆë¬¸ì— ë§ê²Œ ì¬êµ¬ì„±í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ë§Œë“œì„¸ìš”."
            )
            
            answer = response.strip()
            
            # Citationì´ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ì¶”ê°€
            if not self._has_citations(answer):
                answer = self._add_citations_to_answer(answer, search_results)
            
            # ì¶œì²˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì¶”ê°€
            if "**ì°¸ê³  ìë£Œ:**" not in answer and sources:
                answer += f"\n\n**ì°¸ê³  ìë£Œ:**\n" + "\n".join(sources)
            
            return answer
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._generate_fallback_answer(prompt)
    
    def _generate_fallback_answer(self, prompt: str) -> str:
        """KB ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ fallback ë‹µë³€"""
        return f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ Knowledge Baseì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì§ˆë¬¸:** {prompt}

**ìƒí™©:** Knowledge Base ê²€ìƒ‰ì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.

**ì œì•ˆ:**
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”
2. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”
3. ì¼ë°˜ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ KB ì—†ì´ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."""
    
    def _process_simple_response_with_context(self, prompt: str, conversation_history: List[Dict]) -> str:
        """ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ê°„ë‹¨í•œ ì‘ë‹µ ì²˜ë¦¬"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™” (Streamlit í˜¸í™˜ì„±ì„ ìœ„í•´)
            self._last_search_results = []
            
            # ëŒ€í™” ë§¥ë½ êµ¬ì„±
            context_text = ""
            if conversation_history:
                recent_messages = conversation_history[-4:]  # ìµœê·¼ 4ê°œ ë©”ì‹œì§€
                for msg in recent_messages:
                    role = msg.get("role", "")
                    content = msg.get("content", "")
                    if role == "user":
                        context_text += f"ì‚¬ìš©ì: {content}\n"
                    elif role == "assistant":
                        context_text += f"ì–´ì‹œìŠ¤í„´íŠ¸: {content[:200]}...\n"  # 200ìë¡œ ì œí•œ
            
            # ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸
            if context_text:
                full_prompt = f"""ì´ì „ ëŒ€í™” ë§¥ë½:
{context_text}

í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

ìœ„ì˜ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ë‹µë³€ ì§€ì¹¨:
1. ì´ì „ ëŒ€í™”ì˜ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
2. ì—°ì†ì„± ì§ˆë¬¸("ë‹¤ìŒì€?", "ê·¸ëŸ¼?" ë“±)ì˜ ê²½ìš° ì´ì „ ë‹µë³€ì„ ì´ì–´ì„œ ì„¤ëª…
3. ìƒˆë¡œìš´ ì§ˆë¬¸ì´ë¼ë©´ ë…ë¦½ì ìœ¼ë¡œ ë‹µë³€
4. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€

ë‹µë³€:"""
            else:
                full_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
            
            # Cross Region Inferenceìš© ëª¨ë¸ ID ì‚¬ìš©
            model_id = self.config.observation_model
            if "claude-3-5-haiku" in model_id:
                model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
            elif "claude-3-5-sonnet" in model_id:
                model_id = "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            elif "nova-lite" in model_id:
                model_id = "us.amazon.nova-lite-v1:0"
            elif "nova-micro" in model_id:
                model_id = "us.amazon.nova-micro-v1:0"
                
            response = self.bedrock_client.invoke_model(
                model_id=model_id,
                prompt=full_prompt,
                temperature=0.3,
                max_tokens=800,
                system_prompt="ë‹¹ì‹ ì€ ëŒ€í™”ì˜ ë§¥ë½ì„ ì˜ ì´í•´í•˜ê³  ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            )
            
            return response.strip()
            
        except Exception as e:
            self._last_search_results = []  # ì˜¤ë¥˜ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì„¤ì •
            return f"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? (ëŒ€í™” ë§¥ë½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)})"
    
    def _process_simple_response(self, prompt: str) -> str:
        """ê°„ë‹¨í•œ ì‘ë‹µ ì²˜ë¦¬ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return self._process_simple_response_with_context(prompt, [])


def enhanced_mock_tool(func: Callable) -> Callable:
    """í–¥ìƒëœ Mock tool ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            return f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
    
    wrapper.__name__ = func.__name__
    wrapper.__doc__ = func.__doc__
    return wrapper


class EnhancedMockStrandsAgent:
    """í–¥ìƒëœ Mock Strands Agent (ì‹¤ì œ KB ê²€ìƒ‰ ì§€ì›)"""
    
    def __init__(self, config):
        self.config = config
        self.agent = EnhancedMockAgent(config)
        print(f"ğŸš€ Strands Agent ì´ˆê¸°í™” ì™„ë£Œ")
    
    def __call__(self, prompt: str) -> str:
        return self.agent(prompt)
    
    def process_query(self, query: str, conversation_history: List[Dict] = None) -> Dict:
        """
        ReAct íŒ¨í„´ì„ êµ¬í˜„í•œ ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì„œë“œ (Streamlit í˜¸í™˜ì„±)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        conversation_history = conversation_history or []
        
        try:
            # KB ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨ (ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨)
            if not self.agent._needs_kb_search(query, conversation_history):
                # KB ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²½ìš° ì§ì ‘ ë‹µë³€
                result = self.agent._process_simple_response_with_context(query, conversation_history)
                processing_time = time.time() - start_time
                
                return {
                    "final_answer": result,
                    "content": result,
                    "search_results": [],
                    "processing_time": processing_time,
                    "framework": "Strands (Direct)",
                    "iterations": 1,
                    "steps": [
                        {
                            "type": "Direct Response",
                            "content": f"KB ê²€ìƒ‰ ë¶ˆí•„ìš”, ì§ì ‘ ë‹µë³€ ìƒì„±",
                            "model": "Enhanced Mock Agent"
                        }
                    ],
                    "model_info": {
                        "framework": "Strands",
                        "kb_enabled": self.config.is_kb_enabled() if self.config else False
                    },
                    "citations": [],
                    "context_analysis": {
                        "needs_kb_search": False,
                        "has_context": len(conversation_history) > 0
                    }
                }
            
            # ReAct ë£¨í”„ ì‹¤í–‰ (ìµœëŒ€ 5íšŒ ë°˜ë³µ)
            return self._execute_react_loop(query, conversation_history, start_time)
            
        except Exception as e:
            processing_time = time.time() - start_time
            return {
                "final_answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "search_results": [],
                "processing_time": processing_time,
                "framework": "Strands",
                "iterations": 1,
                "steps": [
                    {
                        "type": "Error",
                        "content": str(e),
                        "model": "Enhanced Mock Agent"
                    }
                ],
                "model_info": {
                    "framework": "Strands",
                    "error": True
                },
                "citations": [],
                "context_analysis": {}
            }
    
    def _execute_react_loop(self, query: str, conversation_history: List[Dict], start_time: float) -> Dict:
        """ReAct ë£¨í”„ ì‹¤í–‰"""
        steps = []
        all_search_results = []
        previous_keywords = []
        max_iterations = 5
        
        for iteration in range(1, max_iterations + 1):
            print(f"\nğŸ”„ Strands - ReAct ë°˜ë³µ {iteration}/{max_iterations}")
            
            # 1. Orchestration (í‚¤ì›Œë“œ ìƒì„±)
            orchestration_step = self._orchestration_step(query, previous_keywords, iteration)
            steps.append(orchestration_step)
            
            current_keywords = orchestration_step.get("parsed_result", {}).get("search_keywords", [])
            if not current_keywords:
                break
            
            # 2. Action (KB ê²€ìƒ‰)
            action_step = self._action_step(current_keywords)
            steps.append(action_step)
            
            search_results = action_step.get("search_results", [])
            all_search_results.extend(search_results)
            
            # 3. Observation (í’ˆì§ˆ í‰ê°€)
            observation_step = self._observation_step(query, search_results, iteration)
            steps.append(observation_step)
            
            # ìµœì¢… ë‹µë³€ì¸ì§€ í™•ì¸
            if observation_step.get("parsed_result", {}).get("is_final_answer", False):
                final_answer = observation_step.get("parsed_result", {}).get("final_answer", "")
                break
            
            # ì¬ì‹œë„ í‚¤ì›Œë“œ í™•ì¸
            retry_keywords = observation_step.get("parsed_result", {}).get("retry_keywords", [])
            if retry_keywords:
                previous_keywords.extend(current_keywords)
                continue
            else:
                # ì¬ì‹œë„ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                final_answer = observation_step.get("content", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
        
        # ìµœì¢… ë‹µë³€ì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
        if 'final_answer' not in locals():
            final_answer = "ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë¶€ë¶„ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            if all_search_results:
                final_answer = self._generate_final_answer_from_results(query, all_search_results)
        
        processing_time = time.time() - start_time
        
        return {
            "final_answer": final_answer,
            "content": final_answer,
            "search_results": all_search_results,
            "processing_time": processing_time,
            "framework": "Strands (ReAct)",
            "iterations": iteration,
            "steps": steps,
            "model_info": {
                "framework": "Strands",
                "kb_enabled": self.config.is_kb_enabled() if self.config else False,
                "react_enabled": True
            },
            "citations": self._extract_citations(all_search_results),
            "context_analysis": {
                "needs_kb_search": True,
                "has_context": len(conversation_history) > 0,
                "total_iterations": iteration
            }
        }
    
    def _orchestration_step(self, query: str, previous_keywords: List[str], iteration: int) -> Dict:
        """Orchestration ë‹¨ê³„ - í‚¤ì›Œë“œ ìƒì„±"""
        try:
            if iteration == 1:
                # ì²« ë²ˆì§¸ ë°˜ë³µ: ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.agent._extract_keywords(query)
            else:
                # ì¬ì‹œë„: LLMì„ í†µí•œ ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„±
                keywords = self._generate_retry_keywords_llm(query, previous_keywords)
            
            print(f"   ğŸ¯ ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}")
            
            return {
                "type": "Orchestration",
                "content": f"ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±: {keywords}",
                "model": "Enhanced Mock Orchestration",
                "parsed_result": {
                    "search_keywords": keywords,
                    "iteration": iteration,
                    "intent": f"KB ê²€ìƒ‰ì„ í†µí•œ ì •ë³´ ìˆ˜ì§‘ ({iteration}íšŒì°¨)"
                }
            }
            
        except Exception as e:
            return {
                "type": "Orchestration",
                "content": f"í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {str(e)}",
                "model": "Enhanced Mock Orchestration",
                "parsed_result": {
                    "search_keywords": [],
                    "error": True
                }
            }
    
    def _action_step(self, keywords: List[str]) -> Dict:
        """Action ë‹¨ê³„ - KB ê²€ìƒ‰"""
        try:
            print(f"   âš¡ KB ê²€ìƒ‰ ì‹¤í–‰: {keywords}")
            
            search_results = self.agent.kb_searcher.search_multiple_queries(
                kb_id=self.config.kb_id,
                queries=keywords,
                max_results_per_query=3
            )
            
            print(f"   ğŸ“š ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            
            return {
                "type": "Action",
                "content": f"KB ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼",
                "model": "Enhanced Mock Action",
                "search_results": search_results,
                "parsed_result": {
                    "search_keywords": keywords,
                    "results_count": len(search_results),
                    "search_type": "Knowledge Base"
                }
            }
            
        except Exception as e:
            print(f"   âŒ KB ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return {
                "type": "Action",
                "content": f"KB ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}",
                "model": "Enhanced Mock Action",
                "search_results": [],
                "parsed_result": {
                    "search_keywords": keywords,
                    "results_count": 0,
                    "error": True
                }
            }
    
    def _observation_step(self, query: str, search_results: List[Dict], iteration: int) -> Dict:
        """Observation ë‹¨ê³„ - í’ˆì§ˆ í‰ê°€ ë° ë‹µë³€ ìƒì„±"""
        try:
            # í’ˆì§ˆ í‰ê°€
            quality_assessment = self._assess_search_quality_llm(query, search_results, iteration)
            
            print(f"   ğŸ‘ï¸ í’ˆì§ˆ í‰ê°€: {quality_assessment['reason']} (ì ìˆ˜: {quality_assessment['score']:.3f})")
            
            if iteration >= 5 or not quality_assessment["needs_retry"]:
                # ìµœì¢… ë‹µë³€ ìƒì„±
                final_answer = self._generate_final_answer_from_results(query, search_results)
                
                return {
                    "type": "Observation",
                    "content": final_answer,
                    "model": "Enhanced Mock Observation",
                    "parsed_result": {
                        "is_final_answer": True,
                        "final_answer": final_answer,
                        "needs_retry": False,
                        "quality_score": quality_assessment["score"],
                        "iteration": iteration
                    }
                }
            else:
                # ì¬ì‹œë„ í•„ìš”
                retry_keywords = self._generate_retry_keywords_simple(query, search_results)
                
                return {
                    "type": "Observation",
                    "content": f"ê²€ìƒ‰ ê²°ê³¼ ë¶ˆì¶©ë¶„, ì¬ì‹œë„ í•„ìš”: {retry_keywords}",
                    "model": "Enhanced Mock Observation",
                    "parsed_result": {
                        "is_final_answer": False,
                        "final_answer": "",
                        "needs_retry": True,
                        "retry_keywords": retry_keywords,
                        "quality_score": quality_assessment["score"],
                        "iteration": iteration
                    }
                }
                
        except Exception as e:
            print(f"   âŒ Observation ì˜¤ë¥˜: {str(e)}")
            return {
                "type": "Observation",
                "content": f"í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {str(e)}",
                "model": "Enhanced Mock Observation",
                "parsed_result": {
                    "is_final_answer": True,
                    "final_answer": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "needs_retry": False,
                    "error": True
                }
            }


    def _assess_search_quality_llm(self, query: str, search_results: List[Dict], iteration: int) -> Dict:
        """LLMì„ í†µí•œ ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        if not search_results:
            return {
                "needs_retry": iteration < 5,
                "reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                "score": 0.0
            }
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            results_summary = ""
            for i, result in enumerate(search_results[:3], 1):
                content = result.get('content', '')[:150]
                score = result.get('score', 0)
                results_summary += f"ê²°ê³¼ {i} (ì ìˆ˜: {score:.3f}): {content}...\n"
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ (LLM ì—†ì´)
            avg_score = sum(result.get('score', 0) for result in search_results) / len(search_results)
            
            # ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¥¸ ì„ê³„ê°’
            if iteration <= 2:
                threshold = 0.5
            elif iteration <= 4:
                threshold = 0.4
            else:
                threshold = 0.2
            
            needs_retry = avg_score < threshold and iteration < 5
            
            return {
                "needs_retry": needs_retry,
                "reason": f"í‰ê·  ì ìˆ˜ {avg_score:.3f} (ì„ê³„ê°’: {threshold}, {iteration}íšŒì°¨)",
                "score": avg_score
            }
            
        except Exception as e:
            return {
                "needs_retry": iteration < 5,
                "reason": f"í‰ê°€ ì˜¤ë¥˜: {str(e)}",
                "score": 0.0
            }
    
    def _generate_retry_keywords_llm(self, query: str, previous_keywords: List[str]) -> List[str]:
        """LLMì„ í†µí•œ ì¬ì‹œë„ í‚¤ì›Œë“œ ìƒì„±"""
        try:
            retry_prompt = f"""ì´ì „ ê²€ìƒ‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•œ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

ì›ë³¸ ì§ˆë¬¸: {query}
ì´ì „ ê²€ìƒ‰ í‚¤ì›Œë“œ: {previous_keywords}

ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìƒì„± ì „ëµ:
1. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ì–´ ì‚¬ìš©
2. ë” êµ¬ì²´ì ì´ê±°ë‚˜ ë” ì¼ë°˜ì ì¸ í‘œí˜„ ì‹œë„
3. ë‹¤ë¥¸ ê´€ì ì—ì„œì˜ ì ‘ê·¼
4. ê´€ë ¨ ë¶„ì•¼ì˜ ì „ë¬¸ ìš©ì–´ í™œìš©
5. ì´ì „ í‚¤ì›Œë“œì™€ëŠ” ë‹¤ë¥¸ ìƒˆë¡œìš´ ì ‘ê·¼

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ 3ê°œì˜ ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”:
KEYWORD1: [í‚¤ì›Œë“œ1]
KEYWORD2: [í‚¤ì›Œë“œ2]
KEYWORD3: [í‚¤ì›Œë“œ3]"""

            # Cross Region Inferenceìš© ëª¨ë¸ ID ì‚¬ìš©
            model_id = self.config.observation_model
            if "claude-3-5-haiku" in model_id:
                model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
            elif "claude-3-5-sonnet" in model_id:
                model_id = "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            elif "nova-lite" in model_id:
                model_id = "us.amazon.nova-lite-v1:0"
            elif "nova-micro" in model_id:
                model_id = "us.amazon.nova-micro-v1:0"

            response = self.agent.bedrock_client.invoke_model(
                model_id=model_id,
                prompt=retry_prompt,
                temperature=0.7,  # ë” ì°½ì˜ì ì¸ í‚¤ì›Œë“œ ìƒì„±
                max_tokens=300,
                system_prompt="ë‹¹ì‹ ì€ ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤íŒ¨í•œ ê²€ìƒ‰ì„ ê°œì„ í•˜ê¸° ìœ„í•œ íš¨ê³¼ì ì¸ ëŒ€ì•ˆ í‚¤ì›Œë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            # í‚¤ì›Œë“œ íŒŒì‹±
            new_keywords = []
            lines = response.strip().split('\n')
            
            for line in lines:
                if line.startswith('KEYWORD'):
                    try:
                        keyword = line.split(':', 1)[1].strip()
                        if keyword and keyword not in new_keywords and keyword not in str(previous_keywords):
                            new_keywords.append(keyword)
                    except:
                        continue
            
            # LLM ìƒì„±ì´ ì‹¤íŒ¨í•œ ê²½ìš° í´ë°±
            if not new_keywords:
                print("   âš ï¸ LLM ì¬ì‹œë„ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨, í´ë°± ì‚¬ìš©")
                new_keywords = self._generate_retry_keywords_simple(query, [])
            
            print(f"   ğŸ¤– LLM ì¬ì‹œë„ í‚¤ì›Œë“œ: {new_keywords}")
            return new_keywords[:3]
            
        except Exception as e:
            print(f"   âŒ LLM ì¬ì‹œë„ í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return self._generate_retry_keywords_simple(query, [])
    
    def _generate_retry_keywords_simple(self, query: str, search_results: List[Dict]) -> List[str]:
        """ê°„ë‹¨í•œ ì¬ì‹œë„ í‚¤ì›Œë“œ ìƒì„±"""
        import re
        korean_words = re.findall(r'[ê°€-í£]+', query)
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¡°í•©
        keywords = []
        if len(korean_words) >= 2:
            keywords.append(f"{korean_words[0]} {korean_words[1]}")
        
        for word in korean_words:
            if len(word) >= 2:
                keywords.append(word)
        
        return keywords[:3] if keywords else ["ì •ë³´", "ì •ì±…"]
    
    def _generate_final_answer_from_results(self, query: str, search_results: List[Dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± - LLM ê¸°ë°˜ ì¬ê°€ê³µ"""
        if not search_results:
            return f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ Knowledge Baseì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•˜ì‹œê±°ë‚˜, ê´€ë ¨ ë¶€ì„œì— ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context_parts = []
            sources = []
            
            for i, result in enumerate(search_results[:5], 1):
                content = result.get("content", "")[:400]  # ë” ë§ì€ ë‚´ìš© í¬í•¨
                source = result.get("source", f"ë¬¸ì„œ {i}")
                score = result.get("score", 0)
                
                context_parts.append(f"[{i}] (ê´€ë ¨ì„±: {score:.3f}) {content}")
                sources.append(f"[{i}] {source}")
            
            context = "\n\n".join(context_parts)
            
            # LLMì„ í†µí•œ ë‹µë³€ ì¬ê°€ê³µ
            answer_prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê²€ìƒ‰ ê²°ê³¼:
{context}

ë‹µë³€ ìš”êµ¬ì‚¬í•­:
1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” í˜•íƒœë¡œ ì¬êµ¬ì„±
2. ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë¥¼ í™œìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ [1], [2] í˜•íƒœì˜ Citation í¬í•¨
3. êµ¬ì²´ì ì¸ ì ˆì°¨, ê¸°ì¤€, ì¡°ê±´ ë“±ì´ ìˆë‹¤ë©´ ëª…í™•íˆ ì„¤ëª…
4. ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  ëª…ì‹œ
5. ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
6. ë‹¨ìˆœíˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³  ì§ˆë¬¸ì— ë§ê²Œ ì¬êµ¬ì„±

ë‹µë³€:"""

            # Cross Region Inferenceìš© ëª¨ë¸ ID ì‚¬ìš©
            model_id = self.config.observation_model
            if "claude-3-5-haiku" in model_id:
                model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
            elif "claude-3-5-sonnet" in model_id:
                model_id = "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            elif "nova-lite" in model_id:
                model_id = "us.amazon.nova-lite-v1:0"
            elif "nova-micro" in model_id:
                model_id = "us.amazon.nova-micro-v1:0"

            # agent ê°ì²´ì˜ bedrock_client ì‚¬ìš©
            response = self.agent.bedrock_client.invoke_model(
                model_id=model_id,
                prompt=answer_prompt,
                temperature=0.1,  # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ temperature
                max_tokens=1500,  # ë” ê¸´ ë‹µë³€ í—ˆìš©
                system_prompt="ë‹¹ì‹ ì€ Knowledge Base ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¨ìˆœ ë‚˜ì—´í•˜ì§€ ë§ê³  ì§ˆë¬¸ì— ë§ê²Œ ì¬êµ¬ì„±í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
            )
            
            answer = response.strip()
            
            # Citationì´ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ì¶”ê°€
            if not self._has_citations(answer):
                answer = self._add_citations_to_answer(answer, search_results)
            
            # ì¶œì²˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì¶”ê°€
            if "**ì°¸ê³  ìë£Œ:**" not in answer and sources:
                answer += f"\n\n**ì°¸ê³  ìë£Œ:**\n" + "\n".join(sources)
            
            return answer
            
        except Exception as e:
            print(f"âŒ LLM ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            # í´ë°±: ê¸°ë³¸ ë‹µë³€ ìƒì„±
            return self._generate_fallback_answer_from_results(query, search_results)
    
    def _has_citations(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì— Citationì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        import re
        citation_pattern = r'\[\d+\]'
        return bool(re.search(citation_pattern, text))
    
    def _add_citations_to_answer(self, answer: str, search_results: List[Dict]) -> str:
        """ë‹µë³€ì— Citation ìë™ ì¶”ê°€"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ì˜ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì°¾ì•„ì„œ Citation ì¶”ê°€
            enhanced_answer = answer
            
            for i, result in enumerate(search_results[:3], 1):
                content = result.get("content", "")
                # ê²€ìƒ‰ ê²°ê³¼ì˜ ì£¼ìš” ë‹¨ì–´ë“¤ì„ ì°¾ì•„ì„œ Citation ì¶”ê°€
                import re
                key_phrases = re.findall(r'[ê°€-í£]{2,}', content)[:5]  # ì£¼ìš” í•œê¸€ ë‹¨ì–´ 5ê°œ
                
                for phrase in key_phrases:
                    if phrase in enhanced_answer and f"[{i}]" not in enhanced_answer:
                        enhanced_answer = enhanced_answer.replace(phrase, f"{phrase}[{i}]", 1)
                        break
            
            return enhanced_answer
            
        except Exception as e:
            return answer
    
    def _generate_fallback_answer_from_results(self, query: str, search_results: List[Dict]) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ë‹µë³€ ìƒì„±"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë‚´ìš© ì¶”ì¶œ
            best_results = sorted(search_results, key=lambda x: x.get('score', 0), reverse=True)[:3]
            
            answer = f"'{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
            
            for i, result in enumerate(best_results, 1):
                content = result.get("content", "")[:200]
                source = result.get("source", f"ë¬¸ì„œ {i}")
                answer += f"**{i}. ** {content}... [ì¶œì²˜: {source}]\n\n"
            
            answer += "ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            
            return answer
            
        except Exception as e:
            return f"ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _extract_citations(self, search_results: List[Dict]) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¸ìš© ì •ë³´ ì¶”ì¶œ"""
        citations = []
        for i, result in enumerate(search_results[:5], 1):
            citations.append({
                "id": i,
                "content": result.get("content", "")[:200],
                "source": result.get("source", f"ë¬¸ì„œ {i}"),
                "score": result.get("score", 0)
            })

# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
MockAgent = EnhancedMockAgent
mock_tool = enhanced_mock_tool
