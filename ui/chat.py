"""
ìˆ˜ì •ëœ Streamlit ì±„íŒ… UI ì»´í¬ë„ŒíŠ¸
ReAct Agent ì‘ë‹µ êµ¬ì¡°ì™€ í˜¸í™˜ì„± ê°œì„ 
"""

import streamlit as st
from typing import List, Dict, Any
import time

from utils.config import AgentConfig


def render_chat_interface(config: AgentConfig) -> None:
    """
    ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§
    
    Args:
        config: Agent ì„¤ì •
    """
    # ì±„íŒ… ë©”ì‹œì§€ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    _render_chat_history()
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    _handle_user_input(config)


def _render_chat_history():
    """ì±„íŒ… ê¸°ë¡ í‘œì‹œ"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                # Assistant ë©”ì‹œì§€ì˜ ê²½ìš° ReAct ë‹¨ê³„ ì •ë³´ë„ í‘œì‹œ
                _render_assistant_message(message)
            else:
                # User ë©”ì‹œì§€
                content = message.get("content", "")
                if content:
                    st.write(content)
                else:
                    st.write("(ë©”ì‹œì§€ ë‚´ìš© ì—†ìŒ)")


def _render_assistant_message(message: Dict[str, Any]):
    """Assistant ë©”ì‹œì§€ ë Œë”ë§ (ReAct ë‹¨ê³„ í¬í•¨)"""
    # ìµœì¢… ë‹µë³€ í‘œì‹œ (ì•ˆì „í•œ ë°©ì‹)
    content = message.get("content", message.get("final_answer", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
    st.write(content)
    
    # ReAct ë‹¨ê³„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
    react_steps = message.get("react_steps", message.get("steps", []))
    if react_steps:
        _render_react_steps(react_steps)
    
    # ì‹¤í–‰ ì •ë³´ í‘œì‹œ
    _render_execution_info(message)


def _render_react_steps(react_steps: List[Dict]):
    """ReAct ë‹¨ê³„ë³„ ì •ë³´ í‘œì‹œ"""
    if not react_steps:
        return
        
    with st.expander("ğŸ” ReAct ë‹¨ê³„ë³„ ìƒì„¸ ì •ë³´", expanded=False):
        for i, step in enumerate(react_steps):
            step_type = step.get("type", "Unknown")
            step_content = step.get("content", "")
            
            # ë‹¨ê³„ë³„ ì•„ì´ì½˜ ì„¤ì •
            if step_type == "Orchestration":
                icon = "ğŸ¯"
                color = "blue"
            elif step_type == "Action":
                icon = "âš¡"
                color = "green"
            elif step_type == "Observation":
                icon = "ğŸ‘ï¸"
                color = "orange"
            elif step_type == "Error":
                icon = "âŒ"
                color = "red"
            else:
                icon = "â„¹ï¸"
                color = "gray"
            
            # ë‹¨ê³„ ì •ë³´ í‘œì‹œ
            st.markdown(f"**{icon} {step_type}** (Step {i+1})")
            
            if step_type == "Error":
                st.error(step_content)
            else:
                # ëª¨ë¸ ì •ë³´ í‘œì‹œ
                if "model" in step:
                    model_name = _get_short_model_name(step["model"])
                    st.caption(f"Model: {model_name}")
                
                # íŒŒì‹±ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  í‘œì‹œ
                parsed_result = step.get("parsed_result", {})
                if parsed_result and not parsed_result.get("error", False):
                    _render_parsed_result(step_type, parsed_result)
                
                # ì›ë³¸ ë‚´ìš© í‘œì‹œ (ê¸¸ë©´ ì¶•ì•½) - expander ì¤‘ì²© ë°©ì§€
                if step_content and len(step_content) > 50:
                    st.markdown("**ì›ë³¸ ì‘ë‹µ:**")
                    if len(step_content) > 500:
                        st.text(step_content[:500] + "...")
                        st.caption(f"(ì „ì²´ {len(step_content)}ì ì¤‘ 500ì í‘œì‹œ)")
                    else:
                        st.text(step_content)
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
                if step_type == "Action":
                    search_results = step.get("search_results", parsed_result.get("search_results", []))
                    if search_results:
                        _render_search_results(search_results)
            
            if i < len(react_steps) - 1:
                st.divider()


def _render_parsed_result(step_type: str, parsed_result: Dict):
    """íŒŒì‹±ëœ ê²°ê³¼ í‘œì‹œ"""
    if step_type == "Orchestration":
        intent = parsed_result.get("intent", "")
        keywords = parsed_result.get("search_keywords", [])
        confidence = parsed_result.get("confidence", 0)
        
        if intent:
            st.markdown(f"**ì˜ë„**: {intent}")
        if keywords:
            st.markdown(f"**ê²€ìƒ‰ í‚¤ì›Œë“œ**: {', '.join(keywords)}")
        if confidence:
            st.markdown(f"**ì‹ ë¢°ë„**: {confidence:.2f}")
    
    elif step_type == "Action":
        search_type = parsed_result.get("search_type", "")
        search_keywords = parsed_result.get("search_keywords", [])
        
        if search_type:
            st.markdown(f"**ê²€ìƒ‰ ìœ í˜•**: {search_type}")
        if search_keywords:
            st.markdown(f"**ì‚¬ìš©ëœ í‚¤ì›Œë“œ**: {', '.join(search_keywords)}")
    
    elif step_type == "Observation":
        is_final = parsed_result.get("is_final_answer", False)
        final_answer = parsed_result.get("final_answer", "")
        
        if is_final:
            st.success("âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
        if final_answer and len(final_answer) > 100:
            st.markdown("**ìƒì„±ëœ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°**:")
            st.text(final_answer[:100] + "...")


def _render_search_results(search_results: List[Dict]):
    """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ"""
    if not search_results:
        st.caption("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return
    
    st.caption(f"ğŸ“š ê²€ìƒ‰ ê²°ê³¼ ({len(search_results)}ê°œ)")
    
    for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
        score = result.get('score', 0)
        st.markdown(f"**ê²°ê³¼ {i+1}** (ì ìˆ˜: {score:.3f})")
        
        content = result.get('content', result.get('text', ''))
        if content:
            if len(content) > 200:
                st.text(content[:200] + "...")
            else:
                st.text(content)
        
        source = result.get('source', result.get('metadata', {}).get('source', ''))
        if source:
            st.caption(f"ì¶œì²˜: {source}")
        
        if i < len(search_results[:3]) - 1:
            st.markdown("---")


def _render_execution_info(message: Dict[str, Any]):
    """ì‹¤í–‰ ì •ë³´ í‘œì‹œ"""
    # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
    metadata = message.get("metadata", {})
    
    iterations = metadata.get("total_iterations", message.get("iterations_used", 0))
    max_iterations = 5
    
    if iterations > 0:
        # ì§„í–‰ë¥  í‘œì‹œ
        progress = min(iterations / max_iterations, 1.0)
        st.progress(progress, text=f"ReAct ë°˜ë³µ: {iterations}/{max_iterations}íšŒ")
    
    # ì•ˆì „ì¥ì¹˜ ì‘ë™ ì—¬ë¶€ í‘œì‹œ
    termination_reason = metadata.get("termination_reason", message.get("termination_reason", ""))
    if "ì•ˆì „ì¥ì¹˜" in termination_reason or "ì¤‘ë‹¨" in termination_reason:
        st.warning(f"âš ï¸ {termination_reason}")
    elif termination_reason and termination_reason != "ì •ìƒ ì™„ë£Œ":
        st.info(f"â„¹ï¸ {termination_reason}")
    
    # ì‹¤í–‰ ì‹œê°„ í‘œì‹œ
    total_time = metadata.get("total_time", message.get("execution_time", 0))
    if total_time > 0:
        st.caption(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    # ìµœì í™” ì •ë³´ í‘œì‹œ
    if metadata.get("optimization_level"):
        st.caption(f"ğŸš€ ìµœì í™” ë ˆë²¨: {metadata['optimization_level']}")


def _handle_user_input(config: AgentConfig):
    """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬"""
    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({
            "role": "user", 
            "content": prompt,
            "timestamp": time.time()
        })
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
        with st.chat_message("user"):
            st.write(prompt)
        
        # Assistant ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            _generate_assistant_response(prompt, config)


def _generate_assistant_response(user_input: str, config: AgentConfig):
    """Assistant ì‘ë‹µ ìƒì„± (Strands Agents ë˜ëŠ” Legacy ReAct ì‚¬ìš©)"""
    try:
        # ì‹œìŠ¤í…œ ì„ íƒ í™•ì¸
        use_strands = st.session_state.get('use_strands', True)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸)
        all_messages = st.session_state.messages[:-1]  # ë§ˆì§€ë§‰ ë©”ì‹œì§€(í˜„ì¬ ì…ë ¥) ì œì™¸
        conversation_history = all_messages[-10:] if len(all_messages) > 0 else []
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë” êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
        formatted_history = []
        for msg in conversation_history:
            if msg.get("role") in ["user", "assistant"] and msg.get("content"):
                formatted_history.append({
                    "role": msg["role"],
                    "content": str(msg["content"])[:500],  # ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì¶•ì•½
                    "timestamp": msg.get("timestamp", 0)
                })
        
        if use_strands:
            # Strands Agents ì‹œìŠ¤í…œ ì‚¬ìš©
            _generate_strands_response(user_input, config, formatted_history)
        else:
            # Legacy ReAct ì‹œìŠ¤í…œ ì‚¬ìš©
            _generate_legacy_response(user_input, config, formatted_history)
        
    except Exception as e:
        # ì—ëŸ¬ ì²˜ë¦¬
        error_message = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        st.error(error_message)
        
        # ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ”§ ë””ë²„ê·¸ ì •ë³´", expanded=False):
            st.text(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            st.text(f"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            
            # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í‘œì‹œ
            import traceback
            st.text("ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            st.code(traceback.format_exc())
        
        # ì—ëŸ¬ ì‘ë‹µë„ ì„¸ì…˜ì— ì €ì¥
        error_response = {
            "role": "assistant",
            "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            "timestamp": time.time(),
            "error": True,
            "error_details": str(e)
        }
        st.session_state.messages.append(error_response)


def _generate_strands_response(user_input: str, config: AgentConfig, formatted_history: List[Dict]):
    """Strands Agents ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
    try:
        # Strandsë¥¼ ìš°ì„  ì‚¬ìš© (ì•ˆì •ì„± í™•ë³´)
        try:
            from agents.enhanced_mock_strands import EnhancedMockStrandsAgent
            chatbot = EnhancedMockStrandsAgent(config)
            strands_type = "Strands Agents"
            print("âœ… Strands Agents ì‚¬ìš© (ì‹¤ì œ KB ê²€ìƒ‰ ì§€ì›)")
        except ImportError:
            # í´ë°±: ê°„ì†Œí™”ëœ êµ¬í˜„ ì‚¬ìš©
            from agents.strands_adapter_simple import SimpleStrandsCompatibilityAdapter
            chatbot = SimpleStrandsCompatibilityAdapter(config, use_strands=True)
            strands_type = "Simple Strands Agents"
            print("âš ï¸ Simple Strands Agents ì‚¬ìš©")
        
        with st.spinner(f"ğŸš€ {strands_type}ê°€ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_placeholder = st.empty()
            
            progress_placeholder.progress(0.1, text="ğŸ¯ Context Analysis: ëŒ€í™” ë§¥ë½ ë¶„ì„ ì¤‘...")
            time.sleep(0.5)
            
            progress_placeholder.progress(0.3, text="ğŸ” KB Search: Knowledge Base ê²€ìƒ‰ ì¤‘...")
            time.sleep(0.5)
            
            progress_placeholder.progress(0.7, text="ğŸ“ Answer Generation: ë‹µë³€ ìƒì„± ì¤‘...")
            time.sleep(0.5)
            
            # Strands ì‹œìŠ¤í…œ ì‹¤í–‰
            if hasattr(chatbot, 'process_query'):
                response = chatbot.process_query(user_input, formatted_history)
            else:
                response = chatbot.process_query(user_input, formatted_history)
            
            progress_placeholder.progress(1.0, text=f"âœ… {strands_type} ì™„ë£Œ!")
            time.sleep(0.5)
            progress_placeholder.empty()
            
            # ì‘ë‹µ í‘œì‹œ
            final_answer = response.get("final_answer", response.get("content", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
            st.write(final_answer)
            
            # Strands íŠ¹í™” ì •ë³´ í‘œì‹œ
            _render_strands_info(response)
            
            # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥
            assistant_message = {
                "role": "assistant",
                "content": final_answer,
                "timestamp": time.time(),
                "steps": response.get("steps", []),
                "metadata": response.get("model_info", {}),
                "framework": strands_type,
                "search_results": response.get("search_results", []),
                "citations": response.get("citations", []),
                "iterations": response.get("iterations", 1),
                "processing_time": response.get("processing_time", 0),
                "error": False
            }
            st.session_state.messages.append(assistant_message)
            
    except Exception as e:
        st.error(f"Strands Agents ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        # Legacy ì‹œìŠ¤í…œìœ¼ë¡œ í´ë°±
        st.info("ğŸ”„ Legacy ReAct ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
        _generate_legacy_response(user_input, config, formatted_history)


def _generate_legacy_response(user_input: str, config: AgentConfig, formatted_history: List[Dict]):
    """Legacy ReAct ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
    try:
        from agents.react_agent import ReActAgent
        
        # ReAct Agent ì´ˆê¸°í™”
        react_agent = ReActAgent(config)
        
        with st.spinner("ğŸ”„ Legacy ReAct Agentê°€ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_placeholder = st.empty()
            
            # ì‹¤ì œ ReAct ì—”ì§„ ì‹¤í–‰
            progress_placeholder.progress(0.1, text="ğŸ¯ Orchestration: ì¿¼ë¦¬ ë¶„ì„ ì¤‘...")
            
            response = react_agent.run(user_input, formatted_history)
            
            progress_placeholder.progress(1.0, text="âœ… Legacy ReAct ì™„ë£Œ!")
            time.sleep(0.5)
            progress_placeholder.empty()
            
            # ì‘ë‹µ êµ¬ì¡° ì •ê·œí™”
            final_answer = response.get("final_answer", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì‘ë‹µ í‘œì‹œ
            st.write(final_answer)
            
            # ReAct ë‹¨ê³„ ì •ë³´ í‘œì‹œ
            react_steps = response.get("steps", [])
            if react_steps:
                _render_react_steps(react_steps)
            
            # ì‹¤í–‰ ì •ë³´ í‘œì‹œ
            _render_execution_info(response)
            
            # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥ (ì •ê·œí™”ëœ í˜•íƒœ)
            assistant_message = {
                "role": "assistant",
                "content": final_answer,
                "timestamp": time.time(),
                "steps": react_steps,
                "metadata": response.get("metadata", {}),
                "framework": "Legacy ReAct",
                "error": False
            }
            st.session_state.messages.append(assistant_message)
            
    except Exception as e:
        st.error(f"Legacy ReAct ì˜¤ë¥˜: {str(e)}")
        raise e


def _render_strands_info(response: Dict[str, Any]):
    """Strands Agents íŠ¹í™” ì •ë³´ í‘œì‹œ"""
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°˜ë³µ ì •ë³´
    processing_time = response.get("processing_time", 0)
    iterations = response.get("iterations", 1)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if processing_time > 0:
            st.metric("â±ï¸ ì²˜ë¦¬ ì‹œê°„", f"{processing_time:.2f}ì´ˆ")
    
    with col2:
        if iterations > 0:
            st.metric("ğŸ”„ ë°˜ë³µ íšŸìˆ˜", f"{iterations}íšŒ")
    
    with col3:
        framework = response.get("model_info", {}).get("framework", "Strands Agents")
        st.metric("ğŸš€ í”„ë ˆì„ì›Œí¬", framework)
    
    # ê²€ìƒ‰ ê²°ê³¼ ì •ë³´
    search_results = response.get("search_results", [])
    if search_results:
        with st.expander("ğŸ“š Knowledge Base ê²€ìƒ‰ ê²°ê³¼", expanded=False):
            st.write(f"ì´ {len(search_results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                st.markdown(f"**ê²°ê³¼ {i+1}**")
                
                score = result.get("score", 0)
                st.caption(f"ê´€ë ¨ì„± ì ìˆ˜: {score:.3f}")
                
                content = result.get("content", "")
                if content:
                    if len(content) > 200:
                        st.text(content[:200] + "...")
                    else:
                        st.text(content)
                
                source = result.get("source", "")
                if source:
                    st.caption(f"ì¶œì²˜: {source}")
                
                if i < len(search_results[:3]) - 1:
                    st.divider()
    
    # Citation ì •ë³´
    citations = response.get("citations", [])
    if citations:
        with st.expander("ğŸ“– ì°¸ê³  ìë£Œ", expanded=False):
            for citation in citations:
                citation_id = citation.get("id", "")
                source = citation.get("source", "")
                score = citation.get("score", 0)
                
                st.markdown(f"**[{citation_id}]** {source}")
                if score > 0:
                    st.caption(f"ê´€ë ¨ì„±: {score:.3f}")
    
    # ë§¥ë½ ë¶„ì„ ì •ë³´
    context_analysis = response.get("context_analysis", {})
    if context_analysis:
        with st.expander("ğŸ§  ëŒ€í™” ë§¥ë½ ë¶„ì„", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                if context_analysis.get("is_continuation"):
                    st.success("âœ… ëŒ€í™” ì—°ì†ì„± ì§ˆë¬¸")
                if context_analysis.get("is_greeting"):
                    st.info("ğŸ‘‹ ì¸ì‚¬ë§")
                if context_analysis.get("has_context"):
                    st.info("ğŸ’¬ ì´ì „ ëŒ€í™” ë§¥ë½ ìˆìŒ")
            
            with col2:
                if context_analysis.get("needs_kb_search"):
                    st.info("ğŸ” KB ê²€ìƒ‰ í•„ìš”")
                confidence = context_analysis.get("confidence", 0)
                if confidence > 0:
                    st.metric("ì‹ ë¢°ë„", f"{confidence:.2f}")


def _get_short_model_name(model_id: str) -> str:
    """ëª¨ë¸ IDë¥¼ ì§§ì€ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
    if not model_id:
        return "Unknown"
        
    if "claude-sonnet-4" in model_id:
        return "Claude 4"
    elif "claude-3-7-sonnet" in model_id:
        return "Claude 3.7"
    elif "claude-3-5-sonnet" in model_id:
        return "Claude 3.5 Sonnet"
    elif "claude-3-5-haiku" in model_id:
        return "Claude 3.5 Haiku"
    elif "nova-lite" in model_id:
        return "Nova Lite"
    elif "nova-micro" in model_id:
        return "Nova Micro"
    else:
        return model_id.split(':')[0] if ':' in model_id else model_id
